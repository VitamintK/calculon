{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#x = pd.read_csv(\"len40_mask3.txt\", header=None, delim_whitespace=True, dtype=str, na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = 40\n",
    "vocab_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (text_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le = LabelEncoder()\n",
    "#categorical = le.fit_transform(x.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[i for i in x.values.flatten() if not isinstance(i, str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t = Tokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t.fit_on_sequences(x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t.sequences_to_matrix([[\"Hello\", \"this\", \"is\", \"test\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t.word_index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(filters='\\n', num_words=vocab_size) #try setting vocab_size to 2000 or 4000 if training not working\n",
    "with open(\"len40.txt\") as f:\n",
    "    t.fit_on_texts([f.read(), '<m>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "with open(\"len40_mask3.txt\") as f:\n",
    "    for line in f:\n",
    "        X.append(t.texts_to_sequences([line])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X) #turn our data from python list of lists into np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filled = []\n",
    "with open(\"len40.txt\") as f:\n",
    "    for line in f:\n",
    "        X_filled.append(t.texts_to_sequences([line])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filled = np.array(X_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections import Counter\n",
    "#Counter([len(p) for p in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "OHE = OneHotEncoder()\n",
    "OHE.fit(np.array(list(t.word_index.values())).reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OHE.transform(X_filled[0].reshape((-1,1))).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, t.word_index.items())) #turn our numbers into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.int32' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-1d51eab0c643>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mreverse_word_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#test\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.int32' object is not iterable"
     ]
    }
   ],
   "source": [
    "[reverse_word_map[i] for i in X[0]] #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if vocab_size is None:\n",
    "    vocab_size = len(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g.add(Embedding(vocab_size, 300, input_length=text_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g.add(Bidirectional(LSTM(300, return_sequences=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g.add(TimeDistributed(Dense(vocab_size, activation='softmax')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d.add(Embedding(vocab_size, 300, input_length=text_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, Concatenate\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_input_g = Input(shape=(text_length,vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_layer = Embedding(vocab_size, 300, input_length=text_length)\n",
    "embedding_layer = TimeDistributed(Dense(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = embedding_layer(main_input_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Bidirectional(LSTM(300, return_sequences=True))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_output_g = TimeDistributed(Dense(vocab_size, activation='softmax'))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model = Model(inputs=[main_input_g], outputs=[main_output_g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 40, 97491)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 40, 300)           29247600  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 40, 600)           1442400   \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 40, 97491)         58592091  \n",
      "=================================================================\n",
      "Total params: 89,282,091\n",
      "Trainable params: 89,282,091\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_input_d = Input(shape=(text_length,vocab_size))\n",
    "aux_input_d = Input(shape=(text_length,vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_in = embedding_layer(main_input_d)\n",
    "context = embedding_layer(aux_input_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = Concatenate(axis=-1)([filled_in, context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Bidirectional(LSTM(300, return_sequences=True))(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_output_d = TimeDistributed(Dense(1, activation='sigmoid'))(x) #or sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_model = Model(inputs = [main_input_d, aux_input_d], outputs=[main_output_d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 40, 97491)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 40, 97491)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 40, 300)      29247600    input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 40, 600)      0           time_distributed_1[3][0]         \n",
      "                                                                 time_distributed_1[4][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 40, 600)      2162400     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 40, 1)        601         bidirectional_3[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 31,410,601\n",
      "Trainable params: 31,410,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 40, 97491)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 40, 97491)    89282091    input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_4 (Model)                 (None, 40, 1)        31410601    model_1[3][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 91,445,092\n",
      "Trainable params: 91,445,092\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#AM = Sequential()\n",
    "optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "AM_input = Input(shape=(text_length,vocab_size))\n",
    "generator_output = generator_model(AM_input)\n",
    "discriminator_output = discriminator_model([generator_output, AM_input])\n",
    "AM = Model(inputs=[AM_input], outputs=discriminator_output)\n",
    "#AM.add(discriminator_model)\n",
    "AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "    metrics=['accuracy'])\n",
    "AM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def train(train_steps=2000, batch_size=10, save_interval=1):\n",
    "    def categorical_to_one_hot(categorical):\n",
    "        return OHE.transform(categorical.reshape(batch_size*text_length,1)).toarray().reshape(batch_size,text_length,-1)\n",
    "    noise_input = None\n",
    "    if save_interval>0:\n",
    "        noise_input = np.random.normal(0, 1, size=[16, 100])\n",
    "    for i in range(train_steps):\n",
    "        if i%2 == 0:\n",
    "            gc.collect()\n",
    "        indices_fake = np.random.randint(0, X.shape[0], size=batch_size)\n",
    "        fake_unfilled = X[indices_fake, :]\n",
    "        #noise = np.random.normal(0, 1, size=[batch_size, 100])\n",
    "        fake_unfilled = categorical_to_one_hot(fake_unfilled)\n",
    "        fake_filled = generator_model.predict(fake_unfilled)\n",
    "\n",
    "        indices_real = np.random.randint(0, X.shape[0], size=batch_size)\n",
    "        real_unfilled = X[indices_real, :]\n",
    "        real_filled = X_filled[indices_real, :]\n",
    "        real_unfilled = categorical_to_one_hot(real_unfilled)\n",
    "        real_filled = categorical_to_one_hot(real_filled)\n",
    "        \n",
    "        x_unfilled = np.concatenate((real_unfilled, fake_unfilled))\n",
    "        x_filled = np.concatenate((real_filled, fake_filled))\n",
    "        y = np.ones([2*batch_size, text_length, 1])\n",
    "        y[batch_size:, :, :] = 0\n",
    "        d_loss = discriminator_model.train_on_batch([x_filled, x_unfilled], y)\n",
    "        #above here is training the discriminator (DM)\n",
    "        #under here is training the generator (AM)\n",
    "        for i in range()\n",
    "        y = np.ones([batch_size, text_length, 1])\n",
    "        #noise = np.random.normal(0, 1, size=[batch_size, 100])\n",
    "        indices_fake = np.random.randint(0, X.shape[0], size=batch_size)\n",
    "        fake_unfilled = X[indices_fake, :]\n",
    "        fake_unfilled = categorical_to_one_hot(fake_unfilled)\n",
    "        a_loss = AM.train_on_batch(fake_unfilled, y)\n",
    "        log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "        log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "        print(log_mesg)\n",
    "        if save_interval>0:\n",
    "            if (i+1)%save_interval==0:\n",
    "                print_examples(indices_fake, fake_filled, fake_unfilled, save2file=True)#samples=noise_input.shape[0],\\\n",
    "                    #noise=noise_input, step=(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(indices_fake, fake_filled, fake_unfilled, save2file = False, ):\n",
    "    batch_size = 10\n",
    "    def categorical_to_one_hot(categorical):\n",
    "        return OHE.transform(categorical.reshape(batch_size*text_length,1)).toarray().reshape(batch_size,text_length,-1)\n",
    "\n",
    "    print(' '.join([reverse_word_map.get(OHE.active_features_[np.argmax(i)], '<m>') for i in fake_filled[0]]))\n",
    "    print(' '.join([reverse_word_map.get(i) for i in X_filled[indices_fake][0]]))\n",
    "    print(' '.join([reverse_word_map.get(i) for i in X[indices_fake][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0: [D loss: 0.805859, acc: 0.505000]  [A loss: 0.695342, acc: 0.470000]\n",
      "hobo pie sguardo xzibit xzibit honey honey briefcase briefcase briefcase briefcase ice xzibit xzibit xzibit honey honey honey honey honey xzibit pie pie honey pie sguardo ice pie pie the xzibit xzibit xzibit pie pie xzibit honey honey honey honey\n",
      ", ca n't , stop it , i ca n't stop wo n't stop eos i ca n't , ca n't , stop it , i ca n't , ca n't , stop it eos i ca n't , ca\n",
      ", ca n't , stop it , i ca n't stop wo n't stop eos i ca n't <m> ca n't , stop it , i ca n't , ca n't , <m> <m> eos i ca n't , ca\n",
      "1: [D loss: 0.538994, acc: 0.698750]  [A loss: 0.857328, acc: 0.202500]\n",
      "shoots honey honey honey pie honey honey honey pie briefcase ice ice xzibit xzibit honey pie honey honey honey pie pie pie pie honey pie pie honey pie sguardo xzibit xzibit honey pie pie honey honey honey honey honey honey\n",
      "n't tell eos that i 'd been crying over you crying over you eos since you said so long left me standing all alone alone and crying cryingeos crying crying eos it 's hard to understand that the touch of\n",
      "n't tell eos that <m> 'd been crying over you crying over you eos since you said so long left me standing all <m> alone and crying <m> crying crying eos it 's hard to understand that the touch of\n",
      "2\n",
      "2: [D loss: 0.590286, acc: 0.576250]  [A loss: 0.773435, acc: 0.395000]\n",
      "ice ice ice pie honey honey pie pie pie pie honey honey honey honey pie pie briefcase pie xzibit chris chris pie words words words honey words words words words xzibit honey honey honey honey honey pie honey honey honey\n",
      "i always loved you , eos there 's no one above you , baby . eos do n't tell me it 's over ! eos do n't tell me it 's over ! eos hollywood 's dead , eos elvis\n",
      "i always loved you , eos there 's no one above you , baby . <m> do n't tell me it 's over ! eos do n't <m> me it <m> over ! eos hollywood 's dead , eos elvis\n",
      "3: [D loss: 0.492755, acc: 0.670000]  [A loss: 1.086132, acc: 0.102500]\n",
      "my ice ice watched honey the honey honey ice honey the honey honey honey honey honey honey honey honey honey honey honey honey honey honey hobo ice ice xzibit honey xzibit xzibit ice pie pie pie pie pie pie pie\n",
      "any fool would realize , here comes goodbye eos better take some time to talk it over again with me eos better get all your things together now if you leave eos it 's killin ' me , oh darlin\n",
      "any fool would realize , <m> comes goodbye eos better <m> some time to talk it over again with me eos better get all your things together now <m> you leave eos it 's killin ' me , oh darlin\n",
      "4\n",
      "4: [D loss: 0.423393, acc: 0.798750]  [A loss: 0.943235, acc: 0.370000]\n",
      "my the <m> honey ice honey is is pie is ice is is honey the the the the honey honey honey honey honey the honey honey honey honey the is honey xzibit honey xzibit xzibit xzibit xzibit pie xzibit honey\n",
      "how do i tell her that i 'm gon na wait ? eos when i know that she 'll be begging me to stay . eos and how do i tell her , that the time has come to part\n",
      "how do i tell her that i 'm gon na wait ? eos when i know that she 'll be begging me to stay . <m> <m> <m> do i tell her , that the time has come to part\n",
      "5: [D loss: 0.394343, acc: 0.760000]  [A loss: 1.214457, acc: 0.075000]\n",
      "the the the my the the the the honey my my my is is pie pie pie is honey honey honey is is honey the the the ice ice ice ice the honey pie pie xzibit honey honey honey honey\n",
      "'s what i 'm gon na do eos i 'm gon na write a song eos gon na sing it to everyone eos and then i 'll sing it to you eos 'cos it was you that wrote it too\n",
      "'s what i 'm gon <m> do eos i 'm gon na write a song eos gon na <m> it to everyone eos and then i 'll sing it to <m> eos 'cos it was you that wrote it too\n",
      "6\n",
      "6: [D loss: 0.365273, acc: 0.802500]  [A loss: 1.591646, acc: 0.032500]\n",
      "my ice is is is is the the is honey is the is honey the the now now ice the me is is is is is now now now honey the honey me is is me pie pie honey honey\n",
      "eos to see the distant signs of unforetold eos oh oh , take hold eos from a haze came a rage of thunder eos distant signs of darkness on the way eos fading cries scream of pain and hunger eos\n",
      "eos to see the distant signs of unforetold eos oh oh , take hold eos <m> a haze came a rage of thunder eos distant signs of darkness on the way <m> fading cries <m> of pain and hunger eos\n",
      "7: [D loss: 0.393059, acc: 0.840000]  [A loss: 1.239851, acc: 0.167500]\n",
      "my my my my honey is honey is is is my is is is is is is is is is is is is is is is is is is is is is is is is is is is honey honey\n",
      "it 's not just a love affair , it 's all or nothing eos do i bet it all on love ? or change and freak out ? eos when the chances come my way , will they up and\n",
      "it 's not just a love affair , it 's all or nothing eos do i bet it all on love ? <m> change and freak out ? eos when <m> chances <m> my way , will they up and\n",
      "8\n",
      "8: [D loss: 0.336119, acc: 0.810000]  [A loss: 1.552325, acc: 0.050000]\n",
      "the the my my my is is is is is is is is honey is is is is is is is is is is is is is is is honey now is is honey me is is is honey honey\n",
      "my eyes eos but stil the days seem the same eos and these children that you spit on eos as they try to change their worlds eos are immune to your consultations eos they 're quite aware of what they\n",
      "my eyes eos but stil the days seem the same eos and <m> children that you spit on eos as they try to change their worlds <m> are immune to your <m> eos they 're quite aware of what they\n",
      "9: [D loss: 0.304591, acc: 0.906250]  [A loss: 1.588505, acc: 0.090000]\n",
      "my my the is is is is is is is is is is is is is is is the is is is is is is is is my my is the is is is is is honey honey honey band\n",
      "[ ice cube ] eos yeah , yo , check it eos turn me up a little bit , on the mic eos [ chorus : x2 ] eos i flip shit for gang-bang niggas eos i talk shit for\n",
      "<m> <m> cube ] eos yeah , yo , check it eos turn me up a little <m> , on the mic eos [ chorus : x2 ] eos i flip shit for gang-bang niggas eos i talk shit for\n",
      "10\n",
      "10: [D loss: 0.279815, acc: 0.866250]  [A loss: 1.725228, acc: 0.105000]\n",
      "my the the the the is is is is is is is is is honey honey me me me my is is is is is the is is is is is is is is is is is is is honey\n",
      "still searching for that boy who had the faith to move a mountain . eos want the fire back . eos another question in me eos one for the powers that be eos it 's got me thrown and so\n",
      "still searching for that boy who had the <m> to move a mountain . eos want the fire back <m> eos another question in me eos one for the powers that be eos it 's got me <m> and so\n",
      "11: [D loss: 0.224953, acc: 0.923750]  [A loss: 2.045443, acc: 0.045000]\n",
      "my my my is is the is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is honey\n",
      "when i 'm alone eos i dream on the horizon eos and words fail eos yes , i know there is no light eos in a room where the sun is absent eos if you are not with me eos\n",
      "when i 'm alone eos i dream on the horizon eos and words fail eos yes , i know there is <m> light eos in a room where the <m> is absent eos if you are not with <m> eos\n",
      "12\n",
      "12: [D loss: 0.274072, acc: 0.875000]  [A loss: 1.863224, acc: 0.092500]\n",
      "my the is my my my is is now is is is is is is is is is is is is is now is is the the the the the the is is is is is is my is is\n",
      "eos but it 's too late , too late , too late eos too late for love eos yes it 's too late , too late , it 's too late eos too late for love eos standing by the\n",
      "eos but it 's too late , <m> late , too late eos too late for love <m> yes it 's too late , too late <m> it 's too late eos too late for love eos standing by the\n",
      "13: [D loss: 0.254942, acc: 0.882500]  [A loss: 1.959706, acc: 0.072500]\n",
      "my the the the is the the the is is is is is is is is is is is is now is is is is is is is is is is is is is is is is is is if\n",
      "man eos years ago we used to play eos he used to laugh when i ran away eos but when i fell and hurt my knee eos he used to come and comfort me eos and the pain would go\n",
      "man eos years <m> we used to play eos he used to laugh when i ran away eos but when i fell <m> hurt my knee eos he used to come <m> comfort me eos and the pain would go\n",
      "14\n",
      "14: [D loss: 0.212183, acc: 0.907500]  [A loss: 2.069475, acc: 0.072500]\n",
      "my my the is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is\n",
      "and your fate was in a game of dice they tossed eos there was still that line that you would never cross eos at any cost eos i meant to ask you how you lived what you believed eos with\n",
      "and your fate was in a game of <m> they tossed eos there was still that line that you would never cross eos at any cost eos <m> meant to <m> you how you lived what you believed eos with\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-ed41b399796e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdiscriminator_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m    \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-107-00d5c6df0ea2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_steps, batch_size, save_interval)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mfake_unfilled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices_fake\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mfake_unfilled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategorical_to_one_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_unfilled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0ma_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_unfilled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mlog_mesg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%d: [D loss: %f, acc: %f]\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mlog_mesg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%s  [A loss: %f, acc: %f]\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlog_mesg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1849\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "\n",
    "discriminator_model.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_hot_encoded_test = OHE.transform(fake_unfilled.reshape(400,1)).toarray().reshape(10,40,-1)\n",
    "\n",
    "#[reverse_word_map.get(OHE.active_features_[np.argmax(i)], '<m>') for i in one_hot_encoded_test[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ice sguardo honey honey honey honey hurricane pie pie honey pie pie honey honey honey honey pie pie honey pie pie honey honey honey honey honey honey honey honey honey pie pie pie pie ice xzibit honey honey darlin honey\n",
      "do n't need a crystal ball for me to see clearly eos no astrology or tarot cards eos watching cnn and holding my breath eos to face the daily news scares me to death eos i 'm pessi-mystic eos i\n",
      "do n't need a crystal ball <m> me to see clearly eos no <m> or tarot cards eos watching cnn and holding my breath eos to face the daily news scares <m> to death eos i 'm pessi-mystic eos i\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "def categorical_to_one_hot(categorical):\n",
    "    return OHE.transform(categorical.reshape(batch_size*text_length,1)).toarray().reshape(batch_size,text_length,-1)\n",
    "\n",
    "indices_fake = np.random.randint(0, X.shape[0], size=10)\n",
    "fake_unfilled = X[indices_fake, :]\n",
    "#noise = np.random.normal(0, 1, size=[batch_size, 100])\n",
    "fake_unfilled = categorical_to_one_hot(fake_unfilled)\n",
    "fake_filled = generator_model.predict(fake_unfilled)\n",
    "print(' '.join([reverse_word_map.get(OHE.active_features_[np.argmax(i)], '<m>') for i in fake_filled[0]]))\n",
    "print(' '.join([reverse_word_map.get(i) for i in X_filled[indices_fake][0]]))\n",
    "print(' '.join([reverse_word_map.get(i) for i in X[indices_fake][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = 40\n",
    "vocab_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_shape = (text_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(filters='\\n', num_words=vocab_size) #try setting vocab_size to 2000 or 4000 if training not working\n",
    "with open(\"len40.txt\") as f:\n",
    "    t.fit_on_texts([f.read(), '<m>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "with open(\"len40_mask3.txt\") as f:\n",
    "    for line in f:\n",
    "        X.append(t.texts_to_sequences([line])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X) #turn our data from python list of lists into np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filled = []\n",
    "with open(\"len40.txt\") as f:\n",
    "    for line in f:\n",
    "        X_filled.append(t.texts_to_sequences([line])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filled = np.array(X_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "OHE = OneHotEncoder()\n",
    "OHE.fit(np.array(list(t.word_index.values())).reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OHE.transform(X_filled[0].reshape((-1,1))).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, t.word_index.items())) #turn our numbers into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[reverse_word_map[i] for i in X[0]] #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if vocab_size is None:\n",
    "    vocab_size = len(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, Concatenate\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_input_g = Input(shape=(text_length,vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_layer = Embedding(vocab_size, 300, input_length=text_length)\n",
    "embedding_layer = TimeDistributed(Dense(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = embedding_layer(main_input_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Bidirectional(LSTM(300, return_sequences=True))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_output_g = TimeDistributed(Dense(vocab_size, activation='softmax'))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model = Model(inputs=[main_input_g], outputs=[main_output_g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 40, 97491)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 40, 300)           29247600  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 40, 600)           1442400   \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 40, 97491)         58592091  \n",
      "=================================================================\n",
      "Total params: 89,282,091\n",
      "Trainable params: 89,282,091\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_input_d = Input(shape=(text_length,vocab_size))\n",
    "aux_input_d = Input(shape=(text_length,vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_in = embedding_layer(main_input_d)\n",
    "context = embedding_layer(aux_input_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = Concatenate(axis=-1)([filled_in, context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Bidirectional(LSTM(300, return_sequences=True))(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_output_d = TimeDistributed(Dense(1, activation='sigmoid'))(x) #or sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_model = Model(inputs = [main_input_d, aux_input_d], outputs=[main_output_d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 40, 97491)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 40, 97491)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 40, 300)      29247600    input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 40, 600)      0           time_distributed_1[1][0]         \n",
      "                                                                 time_distributed_1[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 40, 600)      2162400     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 40, 1)        601         bidirectional_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 31,410,601\n",
      "Trainable params: 31,410,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 40, 97491)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 40, 97491)    89282091    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 40, 1)        31410601    model_1[1][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 91,445,092\n",
      "Trainable params: 91,445,092\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#AM = Sequential()\n",
    "optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "AM_input = Input(shape=(text_length,vocab_size))\n",
    "generator_output = generator_model(AM_input)\n",
    "discriminator_output = discriminator_model([generator_output, AM_input])\n",
    "AM = Model(inputs=[AM_input], outputs=discriminator_output)\n",
    "#AM.add(discriminator_model)\n",
    "AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "    metrics=['accuracy'])\n",
    "AM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import ModelCheckpoint\n",
    "# checkpointAM = ModelCheckpoint(\"AM\", verbose=1, save_best_only=False,period=5)\n",
    "# checkpointDM = ModelCheckpoint(\"DM\", verbose=1, save_best_only=False,period=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def train(train_steps=2000, batch_size=10, print_interval=2, save_interval=50):\n",
    "    def categorical_to_one_hot(categorical):\n",
    "        return OHE.transform(categorical.reshape(batch_size*text_length,1)).toarray().reshape(batch_size,text_length,-1)\n",
    "    noise_input = None\n",
    "    if save_interval>0:\n",
    "        noise_input = np.random.normal(0, 1, size=[16, 100])\n",
    "    for i in range(train_steps):\n",
    "        if i%2 == 0:\n",
    "            gc.collect()\n",
    "        indices_fake = np.random.randint(0, X.shape[0], size=batch_size)\n",
    "        fake_unfilled = X[indices_fake, :]\n",
    "        #noise = np.random.normal(0, 1, size=[batch_size, 100])\n",
    "        fake_unfilled = categorical_to_one_hot(fake_unfilled)\n",
    "        fake_filled = generator_model.predict(fake_unfilled)\n",
    "\n",
    "        indices_real = np.random.randint(0, X.shape[0], size=batch_size)\n",
    "        real_unfilled = X[indices_real, :]\n",
    "        real_filled = X_filled[indices_real, :]\n",
    "        real_unfilled = categorical_to_one_hot(real_unfilled)\n",
    "        real_filled = categorical_to_one_hot(real_filled)\n",
    "        \n",
    "        x_unfilled = np.concatenate((real_unfilled, fake_unfilled))\n",
    "        x_filled = np.concatenate((real_filled, fake_filled))\n",
    "        y = np.ones([2*batch_size, text_length, 1])\n",
    "        y[batch_size:, :, :] = 0\n",
    "        d_loss = discriminator_model.train_on_batch([x_filled, x_unfilled], y)\n",
    "        #above here is training the discriminator (DM)\n",
    "        #under here is training the generator (AM)\n",
    "        for j in range(10):\n",
    "            y = np.ones([batch_size, text_length, 1])\n",
    "            #noise = np.random.normal(0, 1, size=[batch_size, 100])\n",
    "            indices_fake = np.random.randint(0, X.shape[0], size=batch_size)\n",
    "            fake_unfilled = X[indices_fake, :]\n",
    "            fake_unfilled = categorical_to_one_hot(fake_unfilled)\n",
    "            a_loss = AM.train_on_batch(fake_unfilled, y)\n",
    "            if a_loss[0] < 1.45:\n",
    "                break\n",
    "        log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "        log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "        print(log_mesg)\n",
    "        if print_interval>0:\n",
    "            if (i+1)%print_interval==0:\n",
    "                print_examples(indices_fake, fake_filled, fake_unfilled, save2file=True)#samples=noise_input.shape[0],\\\n",
    "        if save_interval>0:\n",
    "            if (i+1)%save_interval == 0:\n",
    "                AM.save(\"AM_save.hd5\")\n",
    "                discriminator_model.save(\"DM_save.hd5\")\n",
    "                    #noise=noise_input, step=(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(indices_fake, fake_filled, fake_unfilled, save2file = False, ):\n",
    "    batch_size = 10\n",
    "    def categorical_to_one_hot(categorical):\n",
    "        return OHE.transform(categorical.reshape(batch_size*text_length,1)).toarray().reshape(batch_size,text_length,-1)\n",
    "\n",
    "    print(' '.join([reverse_word_map.get(OHE.active_features_[np.argmax(i)], '<m>') for i in fake_filled[0]]))\n",
    "    print(' '.join([reverse_word_map.get(i) for i in X_filled[indices_fake][0]]))\n",
    "    print(' '.join([reverse_word_map.get(i) for i in X[indices_fake][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "0: [D loss: 0.693241, acc: 0.418750]  [A loss: 0.694094, acc: 0.232500]\n",
      "1: [D loss: 0.691764, acc: 0.500000]  [A loss: 0.693038, acc: 0.562500]\n",
      "rioch exploitation underwater obsessed incoherence incoherence aught wineeos skeddle-daddle-do polow d-tay d-tay curing madd gleeful lovelace lovelace dictu sneer sneer suci detox bandana'ed tacs gibs masachistic u.k.p.i.c hooley quacking 'fool unpredictability remedy all-canadian doubleeos spurt nahihirapan fucker when guaruntee heheos\n",
      "gang of competition eos and i 'm not telling how i reached my number one position eos but four ninety nine and they are wasting their time with my baby eos and here 's the reason why they have my\n",
      "gang of competition eos and i <m> not telling how i reached my number <m> position eos but four <m> nine and they are wasting their time with my baby eos and here 's the reason why they have my\n",
      "2: [D loss: 0.690122, acc: 0.500000]  [A loss: 0.692991, acc: 0.495000]\n",
      "3: [D loss: 0.686294, acc: 0.500000]  [A loss: 0.691712, acc: 0.675000]\n",
      "linwood linwood detox linwood rhododendron wellllll a-m somewhat garbage ribbit identity wellllll wellllll phc slaim wellllll jottin ohoho ohoho experiments ohoho babtised d'ilarit replacin generalised down'n be olden olden olden olden olden fucker underwater boxed underwater fucker masochistic aheah another\n",
      ", i 'm falling in love with you eos i belong to another whose arms have grown cold eos but i promised for ever to have and to hold eos i could never be free dear eos but when i\n",
      ", i 'm falling in love with you eos i belong to another whose arms <m> grown cold eos but i promised for ever to have <m> to hold eos i could <m> be free dear eos but when i\n",
      "4: [D loss: 0.683836, acc: 0.500000]  [A loss: 0.693590, acc: 0.420000]\n",
      "5: [D loss: 0.679978, acc: 0.500000]  [A loss: 0.696028, acc: 0.235000]\n",
      "olden lord holocaust olden olden we full-eos eos we we we olden olden olden olden 'm 'm be holocaust 'm 's olden olden olden a-shovin someone olden olden we olden olden someone labeled labeled holocaust eos someone ballad eos eos\n",
      "apparent eos yeah i get the bluff if i 'll get out eos no room for three , you , me and your doubts eos one of us has got to leave eos and it sure will be me eos\n",
      "apparent eos yeah i get the bluff if i 'll get out <m> no room for three , you , me <m> your doubts eos one of us has got to leave <m> and it sure will be me eos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kevin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-36-739a7451223d>\", line 5, in <module>\n",
      "    train(train_steps=10000)\n",
      "  File \"<ipython-input-34-42ccdbf08a60>\", line 46, in train\n",
      "    discriminator_model.save(\"DM_save\")\n",
      "  File \"C:\\Users\\kevin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\", line 2573, in save\n",
      "    save_model(self, filepath, overwrite, include_optimizer)\n",
      "  File \"C:\\Users\\kevin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\", line 119, in save_model\n",
      "    topology.save_weights_to_hdf5_group(model_weights_group, model_layers)\n",
      "  File \"C:\\Users\\kevin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\", line 2892, in save_weights_to_hdf5_group\n",
      "    param_dset[:] = val\n",
      "  File \"h5py\\_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py\\_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"C:\\Users\\kevin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\_hl\\dataset.py\", line 632, in __setitem__\n",
      "    self.id.write(mspace, fspace, val, mtype, dxpl=self._dxpl)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kevin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kevin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\kevin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\kevin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\kevin\\Anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1459, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\kevin\\Anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1417, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\kevin\\Anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 677, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\kevin\\Anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 713, in getmodule\n",
      "    for modname, module in list(sys.modules.items()):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "\n",
    "discriminator_model.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "train(train_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_hot_encoded_test = OHE.transform(fake_unfilled.reshape(400,1)).toarray().reshape(10,40,-1)\n",
    "\n",
    "#[reverse_word_map.get(OHE.active_features_[np.argmax(i)], '<m>') for i in one_hot_encoded_test[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ice sguardo honey honey honey honey hurricane pie pie honey pie pie honey honey honey honey pie pie honey pie pie honey honey honey honey honey honey honey honey honey pie pie pie pie ice xzibit honey honey darlin honey\n",
      "do n't need a crystal ball for me to see clearly eos no astrology or tarot cards eos watching cnn and holding my breath eos to face the daily news scares me to death eos i 'm pessi-mystic eos i\n",
      "do n't need a crystal ball <m> me to see clearly eos no <m> or tarot cards eos watching cnn and holding my breath eos to face the daily news scares <m> to death eos i 'm pessi-mystic eos i\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 10\n",
    "# def categorical_to_one_hot(categorical):\n",
    "#     return OHE.transform(categorical.reshape(batch_size*text_length,1)).toarray().reshape(batch_size,text_length,-1)\n",
    "\n",
    "# indices_fake = np.random.randint(0, X.shape[0], size=10)\n",
    "# fake_unfilled = X[indices_fake, :]\n",
    "# #noise = np.random.normal(0, 1, size=[batch_size, 100])\n",
    "# fake_unfilled = categorical_to_one_hot(fake_unfilled)\n",
    "# fake_filled = generator_model.predict(fake_unfilled)\n",
    "# print(' '.join([reverse_word_map.get(OHE.active_features_[np.argmax(i)], '<m>') for i in fake_filled[0]]))\n",
    "# print(' '.join([reverse_word_map.get(i) for i in X_filled[indices_fake][0]]))\n",
    "# print(' '.join([reverse_word_map.get(i) for i in X[indices_fake][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
